{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rides and the Poisson Model\n",
    "\n",
    "To help the urban planners, you are called to model the daily bike rides in NYC using [this dataset](https://gist.github.com/sachinsdate/c17931a3f000492c1c42cf78bf4ce9fe/archive/7a5131d3f02575668b3c7e8c146b6a285acd2cd7.zip).  The dataset contains date, day of the week, high and low temp, precipitation and bike ride couunts as columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood I \n",
    " \n",
    "The obvious choice in distributions is the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) which depends only on one parameter, Î», which is the average number of occurrences per interval. We want to estimate this parameter using Maximum Likelihood Estimation.\n",
    "\n",
    "Implement a Gradient Descent algorithm from scratch that will estimate the Poisson distribution according to the Maximum Likelihood criterion. Plot the estimated mean vs iterations to showcase convergence towards the true mean. \n",
    "\n",
    "References: \n",
    "\n",
    "1. [This blog post](https://towardsdatascience.com/the-poisson-process-everything-you-need-to-know-322aa0ab9e9a). \n",
    "\n",
    "2. [This blog post](https://towardsdatascience.com/understanding-maximum-likelihood-estimation-fa495a03017a) and note the negative  log likelihood function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-246604.20560747295\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "learning_rate=0.1\n",
    "num_iters=1000\n",
    "data=pd.read_csv(\"nyc_bb_bicyclist_counts.csv\")\n",
    "n=data.shape[0]\n",
    "# print(data)\n",
    "bb_count=np.array(data['BB_COUNT'])\n",
    "# print(bb_count)\n",
    "lambda1=0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    lambda1 = lambda1-learning_rate*(-n+bb_count.sum()/n)\n",
    "\n",
    "print(lambda1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood II\n",
    "\n",
    "A colleague of yours suggest that the parameter $\\lambda$ must be itself dependent on the weather and other factors since people bike when its not raining. Assume that you model $\\lambda$ as \n",
    "\n",
    "$$\\lambda_i = \\exp(\\mathbf w^T \\mathbf x_i)$$\n",
    "\n",
    "where $\\mathbf x_i$ is one of the example features and $\\mathbf w$ is a set of parameters. \n",
    "\n",
    "Train the model with SGD with this assumption and compare the MSE of the predictions with the `Maximum Likelihood I` approach. \n",
    "\n",
    "You may want to use [this partial derivative of the log likelihood function](http://home.cc.umanitoba.ca/~godwinrt/7010/poissonregression.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
